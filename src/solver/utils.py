import math
import sys
from typing import Dict, Iterable, List

import numpy as np
import torch
import torch.amp
from torch.cuda.amp.grad_scaler import GradScaler
from torch.utils.tensorboard import SummaryWriter
import argparse
from ..data import CocoEvaluator
from ..data.dataset import mscoco_category2label
from ..misc import MetricLogger, SmoothedValue, dist_utils, save_samples
from ..optim import ModelEMA, Warmup
from .validator import Validator, scale_boxes
import os
import cv2
from ..zoo.dfine.box_ops import box_cxcywh_to_xyxy
from PIL import Image
import matplotlib.pyplot as plt


def save_results_with_depth(results, img_paths, output_dir,  class_names=None):
    """
    å°†æ£€æµ‹ç»“æœä»¥ MOT æ ¼å¼ + æ·±åº¦ä¿å­˜åˆ°åœºæ™¯ txt æ–‡ä»¶ä¸­
    results: List[Dict]  # postprocessor è¾“å‡º
    img_paths: List[str] # å½“å‰ batch çš„å›¾åƒè·¯å¾„
    depth_map: torch.Tensor  # å½“å‰ batch çš„æ·±åº¦å›¾ (B,H,W)
    frame_idx: int  # å½“å‰å¸§ç¼–å·
    """
    os.makedirs(output_dir, exist_ok=True)
    
    for res, img_path in zip(results, img_paths):
        frame_name = os.path.splitext(os.path.basename(img_path))[0]
        # è½¬æ¢ä¸º int
        frame_idx = int(frame_name)-1

        scne = img_path.split("/")[-3]  # åœºæ™¯å
        save_path = os.path.join(output_dir, f"{scne}.txt")
        box_depth = res['depths'].cpu().numpy()*25
        boxes = res["boxes"].cpu().numpy()
        scores = res["scores"].cpu().numpy()
        # labels = res["labels"].cpu().numpy()

        with open(save_path, "a") as f:
            for box, score, depth in zip(boxes, scores, box_depth):
                if score>=0.5:
                    # xyxy -> xywh
                    x1, y1, x2, y2 = box
                    w, h = x2 - x1, y2 - y1
                    x, y = x1, y1

                    # MOT æ ¼å¼: frame_id, track_id, x, y, w, h, score, -1, -1, -1, depth
                    line = f"{frame_idx}, -1, {x:.2f}, {y:.2f}, {w:.2f}, {h:.2f}, {score:.4f}, 1, 1, 1, {depth:.4f}\n"
                    f.write(line)


def draw_track_results(img, online_targets, depth_map=None, class_names=None, show_depth_overlay=False):
    """
    ç»˜åˆ¶è·Ÿè¸ªæ¡†å’Œæ·±åº¦ä¿¡æ¯ï¼Œå¯é€‰æ‹©æ˜¯å¦å åŠ æ·±åº¦å›¾
    Args:
        img: è¾“å…¥å›¾åƒï¼ŒPIL.Image æˆ– torch.Tensor æˆ– np.array
        online_targets: å½“å‰å¸§è·Ÿè¸ªç›®æ ‡åˆ—è¡¨
        depth_map: å¯é€‰ï¼Œæ·±åº¦å›¾ (H, W) np.array
        class_names: å¯é€‰ï¼Œç±»åˆ«åç§°åˆ—è¡¨
        show_depth_overlay: æ˜¯å¦åœ¨å›¾åƒä¸Šå åŠ æ·±åº¦å›¾
    Returns:
        ç»˜åˆ¶åçš„å›¾åƒ (RGB)
    """
    # è½¬ä¸º np.array
    if isinstance(img, Image.Image):
        img = np.array(img)
    elif isinstance(img, torch.Tensor):
        img = img.cpu().numpy().transpose(1, 2, 0)
        img = (img * 255).astype(np.uint8)

    H, W = img.shape[:2]

    # å åŠ æ·±åº¦å›¾
    if depth_map is not None and show_depth_overlay:
        depth_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        depth_colored = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
        img = cv2.addWeighted(img, 0.6, depth_colored, 0.4, 0)

    # éšæœºé¢œè‰²
    np.random.seed(42)
    colors = {i: (int(np.random.randint(0, 255)),
                  int(np.random.randint(0, 255)),
                  int(np.random.randint(0, 255))) for i in range(1000)}

    for target in online_targets:
        track_id = target.track_id
        x, y, w, h = target.tlwh
        x1 = max(0, int(x))
        y1 = max(0, int(y))
        x2 = min(W - 1, int(x + w))
        y2 = min(H - 1, int(y + h))

        color = colors[track_id % 1000]
        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)

        label_name = "person"
        score = getattr(target, "depth", 1.0)*25
        text = f"ID:{track_id} {label_name} {score:.2f}"

        (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
        ty1 = max(0, y1 - th - 4)
        cv2.rectangle(img, (x1, ty1), (x1 + tw + 4, y1), color, -1)
        cv2.putText(img, text, (x1 + 2, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

    return img

def visualize_track_results(img_paths, track_results, depth_maps=None, output_dir="./track_vis", class_names=None, show_depth_overlay=False):
    """
    æ‰¹é‡å¯è§†åŒ–è·Ÿè¸ªç»“æœå’Œæ·±åº¦å›¾
    Args:
        img_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
        track_results: è·Ÿè¸ªç›®æ ‡åˆ—è¡¨
        depth_maps: å¯é€‰ï¼Œæ·±åº¦å›¾åˆ—è¡¨ (ä¸ img_paths å¯¹åº”)
        output_dir: ä¿å­˜ç›®å½•
        class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        show_depth_overlay: æ˜¯å¦å åŠ æ·±åº¦å›¾åˆ°è·Ÿè¸ªå›¾
        save_depth_map: æ˜¯å¦å•ç‹¬ä¿å­˜æ·±åº¦å›¾
    """
    os.makedirs(output_dir, exist_ok=True)
    depth_dir = os.path.join(output_dir, "depth_maps")
    track_dir = os.path.join(output_dir,'track_vis')
    os.makedirs(depth_dir, exist_ok=True)
    os.makedirs(track_dir, exist_ok=True)
    if depth_maps is not None:
        save_depth_map = True
        depth_maps=depth_maps.cpu().detach().numpy()
    else:
        save_depth_map = False
    if save_depth_map:
        os.makedirs(depth_dir, exist_ok=True)

    for idx, (img_path, targets) in enumerate(zip(img_paths, track_results)):
        if not os.path.exists(img_path):
            print(f"Warning: å›¾åƒè·¯å¾„ä¸å­˜åœ¨ {img_path}ï¼Œè·³è¿‡å¯è§†åŒ–")
            continue

        img = Image.open(img_path).convert("RGB")
        depth_map = None
        if depth_maps is not None:
            depth_map = depth_maps[idx]

        # ç»˜åˆ¶è·Ÿè¸ªæ¡†
        drawn_img = draw_track_results(img, targets, depth_map=depth_map, class_names=class_names, show_depth_overlay=show_depth_overlay)
        scne=img_path.split('/')[-3]
        img_name = os.path.basename(img_path)
        save_path = os.path.join(track_dir ,scne, img_name)
        os.makedirs(os.path.join(track_dir ,scne), exist_ok=True)
        drawn_img = cv2.cvtColor(drawn_img, cv2.COLOR_RGB2BGR)
        cv2.imwrite(save_path, drawn_img)
        print(f"è·Ÿè¸ª+æ·±åº¦å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

        # å•ç‹¬ä¿å­˜æ·±åº¦å›¾
        if save_depth_map and depth_map is not None:
            depth_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
            depth_colored = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
            depth_save_path = os.path.join(depth_dir,scne, img_name)
            cv2.imwrite(depth_save_path, depth_colored)
            print(f"æ·±åº¦å›¾å·²å•ç‹¬ä¿å­˜: {depth_save_path}")



def track(outputs, tracker, info_imgs):
    online_targets=[]
    for i, dets in enumerate(outputs):
        info_img=np.array(info_imgs[i])
        # np.array(img_size[i])
        online_target = tracker.update( dets['bytesort_input'].cpu(), info_img, info_img)
        online_targets.append(online_target)
        
        # else:
        #     # å¦‚æœæ²¡æœ‰æ£€æµ‹ç»“æœï¼Œä¼ å…¥ç©ºåˆ—è¡¨
        #     online_targets = tracker.update([], info_imgs, img_size)

    return online_targets


def get_box_depth(targets, pred_depth):

    for i in range(len(targets)):
        # pred_depth[i] shape: [H, W] or [1, H, W]
        depth_map = pred_depth[i]
        if depth_map.ndim == 3:
            depth_map = depth_map.squeeze(0)  # å»æ‰é€šé“ç»´åº¦ if [1, H, W]

        depth_list = []
        boxes = box_cxcywh_to_xyxy(targets[i]["boxes"])
        for t, box in enumerate(boxes):  # åº”è¯¥æ˜¯ [x1, y1, x2, y2]
            x1, y1, x2, y2 = [coord for coord in box]

            # è¾¹ç•Œåˆæ³•æ€§æ£€æŸ¥ï¼Œé˜²æ­¢è¶Šç•Œ
            h, w = depth_map.shape
            x1 = int(max(0, min(x1*w, w - 1)))
            x2 = int(max(0, min(x2*w, w)))
            y1 = int(max(0, min(y1*h, h - 1)))
            y2 = int(max(0, min(y2*h, h)))

            if x2 <= x1 or y2 <= y1:
                continue  # è·³è¿‡æ— æ•ˆ box

            # è·å–è¾¹ç•Œæ¡†åŒºåŸŸçš„æ·±åº¦
            region = depth_map[y1:y2, x1:x2]
            if region.numel() == 0:
                continue  # ç©ºåŒºåŸŸï¼Œè·³è¿‡

            # è®¡ç®—æ·±åº¦å‡å€¼ï¼ˆä¹Ÿå¯ä»¥é€‰æ‹©ä¸­å€¼ï¼‰
            mean_depth = region.mean()

            # æ›´æ–° target ä¸­çš„ depth å€¼
            depth_list.append(mean_depth.item())
        depth_list=torch.tensor(depth_list, device=depth_map.device)
        targets[i]["depth"] = depth_list
        targets[i]['bytesort_input'] =torch.cat([targets[i]['boxes'], torch.ones_like(depth_list).unsqueeze(1), depth_list.unsqueeze(1)],dim=1)
    return targets



from collections import defaultdict
def visualize_depth_tracks(all_online_targets, save_path="depth_tracks.png"):
    """
    å¯è§†åŒ–è·Ÿè¸ªå®ä¾‹çš„è¾¹ç•Œæ¡†ä¸­å¿ƒ x åæ ‡ vs æ·±åº¦å€¼ (2D æŠ˜çº¿å›¾)
    
    Args:
        all_online_targets: listï¼Œæ¯ä¸€å¸§çš„ online_targetsï¼ˆBYTETracker.update çš„è¾“å‡ºï¼‰
        save_path: ä¿å­˜å›¾ç‰‡è·¯å¾„
    """
    # ä¿å­˜æ¯ä¸ª track_id çš„è½¨è¿¹ç‚¹
    track_data = defaultdict(list)  # {track_id: [(center_x, depth), ...]}
        
    track_data = defaultdict(list)  # {track_id: [(frame_idx, depth), ...]}
    frame_idx=0
    for batch_targets in enumerate(all_online_targets):
        for frame_targets in batch_targets:
            frame_idx+=1
            for t in frame_targets:
                depth = getattr(t, "depth", None) 
                if depth is None:
                    continue
                track_data[t.track_id].append((frame_idx, depth))

    plt.figure(figsize=(10, 6))
    for tid, points in track_data.items():
        points = sorted(points, key=lambda p: p[0])  # æŒ‰å¸§å·æ’åº
        xs = [p[0] for p in points]  # frame_id
        ys = [p[1] for p in points]  # depth
        plt.plot(xs, ys, marker='o', markersize=2,label=f'ID {tid}')


    plt.xlabel("Bounding Box Center X")
    plt.ylabel("Depth")
    plt.title("Track Instance Depth Visualization")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"âœ… å¯è§†åŒ–ä¿å­˜åˆ° {save_path}")

def visualize_depth_video(all_online_targets, save_path="./depth_scatter.mp4", fps=10):
    """
    ç”Ÿæˆè§†é¢‘ï¼Œå±•ç¤ºæ¯ä¸€å¸§ä¸­ç›®æ ‡ä¸­å¿ƒ x ä¸æ·±åº¦å€¼çš„æ•£ç‚¹å›¾, å¯è§†åŒ–åœ¨ä¸åŒå¸§ä¸­æ·±åº¦å€¼çš„å˜åŒ–è½¨è¿¹

    Args:
        all_online_targets: listï¼Œæ¯ä¸€å¸§çš„ online_targetsï¼ˆBYTETracker.update çš„è¾“å‡ºï¼‰
        save_path: ä¿å­˜è§†é¢‘è·¯å¾„
        fps: è¾“å‡ºè§†é¢‘çš„å¸§ç‡
    """
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    # å…ˆç»Ÿè®¡å¸§æ•°
    num_frames = len(all_online_targets)

    # ç”»å¸ƒå¤§å°
    fig, ax = plt.subplots(figsize=(8, 6))
    plt.close(fig)

    # ç”¨ cv2.VideoWriter è¾“å‡ºè§†é¢‘
    writer = None

    for frame_idx, batch_targets in enumerate(all_online_targets):
        ax.clear()
        ax.set_xlim(0, 1920)   # è¿™é‡Œå‡è®¾å›¾åƒå®½åº¦ 1920ï¼Œä½ å¯ä»¥æ”¹æˆå®é™…å¤§å°
        ax.set_ylim(0, 50.0)    # å‡è®¾æ·±åº¦å€¼èŒƒå›´ [0,1]ï¼Œå¯æ ¹æ®å®é™…æ”¹
        ax.set_xlabel("Bounding Box Center X")
        ax.set_ylabel("Depth")
        ax.set_title(f"Frame {frame_idx}")

        # åœ¨å½“å‰å¸§ç”»ç‚¹
        for frame_targets in batch_targets:
            for t in frame_targets:
                depth = getattr(t, "depth", None)
                if depth is None:
                    continue
                x, y, w, h = t.tlwh
                center_x = x + w / 2
                ax.scatter(center_x, depth, label=f"ID {t.track_id}", s=40)

        # ä¿å­˜ matplotlib å›¾åˆ° numpy æ•°ç»„
        fig.canvas.draw()
        # æ­£ç¡®çš„è½¬æ¢æ­¥éª¤
        img = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)
        # å…ˆè½¬ä¸º4é€šé“æ ¼å¼
        img = img.reshape(fig.canvas.get_width_height()[::-1] + (4,))
        # å»æ‰Alphaé€šé“ï¼Œå¹¶è½¬æ¢ä¸ºRGBæ ¼å¼
        img = img[:, :, 1:]  # ä¿ç•™RGBé€šé“ï¼Œä¸¢å¼ƒAlpha

        # åˆå§‹åŒ– VideoWriter
        if writer is None:
            h, w, _ = img.shape
            writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))

        # è½¬æ¢é¢œè‰² (matplotlib RGB â†’ OpenCV BGR)
        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        writer.write(img_bgr)

    if writer is not None:
        writer.release()

    print(f"ğŸ¥ æ·±åº¦å¯è§†åŒ–è§†é¢‘å·²ä¿å­˜åˆ° {save_path}")